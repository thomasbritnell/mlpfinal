{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tr_jEBnh-jv"
   },
   "source": [
    "# Title:\n",
    "\n",
    "#### Group Member Names :\n",
    "\n",
    "Anselm Che Fon\n",
    "\n",
    "Thomas Britnell\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeKSxMvrh-j0"
   },
   "source": [
    "### INTRODUCTION:\n",
    "*********************************************************************************************************************\n",
    "#### AIM :\n",
    "\n",
    "Detecting \"fake news\" or deceptive statements using various nlp techniques. Both traditional, neural net, and tranformer tecniques were used in the original paper. Our aim was to see if we could improve upon their claimed 93% accuracy using traditional models (Naive Bayes) by adding Random Forest. \n",
    "\n",
    "*********************************************************************************************************************\n",
    "#### Github Repo:\n",
    "\n",
    "Original:\n",
    "\n",
    "\n",
    "https://github.com/JunaedYounusKhan51/FakeNewsDetection/tree/master/Codes\n",
    "\n",
    "Ours:\n",
    "\n",
    "https://github.com/thomasbritnell/mlpfinal\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*********************************************************************************************************************\n",
    "#### DESCRIPTION OF PAPER:\n",
    "\n",
    "\"A benchmark study of machine learning models for online fake news detection\" by Junaed Younus Khan et al., published in Machine Learning with Applications (2021)\n",
    "\n",
    "This paper evaluates various traditional, deep learning, and advanced pre-trained language models for detecting \"fake news\" across three datasets: Liar, Fake or Real News, and Combined Corpus. The aim of the paper is to outline the options that one has when pursuing an AI model for this purpose that will align with their needs.\n",
    "\n",
    "\n",
    "\n",
    "*********************************************************************************************************************\n",
    "#### PROBLEM STATEMENT :\n",
    "\n",
    "In addition to replicating the results with the traditional ML models used by the paper, we aimed to use Random Forest on the datasets, along with several feature engineering techniques that weren't attempted in the original. The aim is not to try to beat the heavy hitting pre-trained models which clearly performed best in the study. Our goal was to try to outperform or match Naive Bayes on the datasets used, since the paper claims Naive Bayes was the best of the ML models. This could potentially add another option for those looking to integrate fact checking AI into their applications who have resource contraints. \n",
    "\n",
    "*********************************************************************************************************************\n",
    "#### CONTEXT OF THE PROBLEM:\n",
    "\n",
    "The issue at hand is that online statements, especially from \"alternative\" news sources are not accountable to standards of fact checking that might come from more reputable publications. With distrust in the mainstream media, more and more people are seeking out such alternative media sources and aren't thinking critically about the claims they read. Artificial Intelligence, specifically Sentiment Analysis has been propsed as a way to decipher false claims from true ones. There are more or less two distinct categories of AI capable of accomplishing this task. While newer models like transformers perform better than the traditional ones, the article talks about how traditional ML models are still important because they cater to limited hardware. Things like personal blogs might benefit from fact checking, but don't have the resources to run heavy neural networks or transformer models.  \n",
    "\n",
    "*********************************************************************************************************************\n",
    "#### SOLUTION:\n",
    "We propose the implementation of a Random Forest classifier enhanced by extensive feature engineering, including text vectorization and statistical linguistic features. We use SMOTE to handle class imbalance and apply GridSearchCV for hyperparameter tuning to achieve optimal performance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77PIPLQ-h-j1"
   },
   "source": [
    "# Background\n",
    "*********************************************************************************************************************\n",
    "\n",
    "\n",
    "|Reference|Explanation|Dataset/Input|Weakness|\n",
    "|------|------|------|------|\n",
    "\n",
    "Reference:\n",
    "https://www.sciencedirect.com/science/article/pii/S266682702100013X?ref=cra_js_challenge&fr=RR-1\n",
    "\n",
    "Explanation:\n",
    "\n",
    "\n",
    "The Datasets:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Weaknesses : \n",
    "\n",
    "The paper fundametnally assumes that fake news or false statements can be detected by sentiment analysis methods. This assumes that these statements have inherit truth based on their structure or the words that they use; that they can be detected without cross verifying the facts involed. In other words, the models can only ever at best detect if text is written in a way that sounds true, not if it is true or not. Similarly, models like this could be reverse engineered by those looking to disseminate false information, to help them form sentences that appear more true. It is presumably much more reliable to cross reference statements of fact in order to see if they are true. \n",
    "*********************************************************************************************************************\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deODH3tMh-j2"
   },
   "source": [
    "# Implement paper code :\n",
    "*********************************************************************************************************************\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start.........\n",
      ".\n",
      "nb start\n",
      "nb done\n",
      "###################\n",
      ".\n",
      "test results: \n",
      "---------nb---------------------\n",
      "test_accuracy: \n",
      "0.5966\n",
      "test_precision: \n",
      "0.5973\n",
      "test_recall: \n",
      "0.5966\n",
      "test_f1 \n",
      "0.5758\n"
     ]
    }
   ],
   "source": [
    "# This is just a subset of the original code, showcasing how Naive Bayes was implemented for the \"Liar\" dataset.\n",
    "#In the full repository they use Naive Bayes using bigram and unigram, for all three datasets. \n",
    "# We also did Random Forest with our feature engineering methods on the other datasets \n",
    "\n",
    "# We renamed get_feature_names to get_feature_names_out to reflect the newer function name from tf-idf\n",
    "\n",
    "# NOTE: this creates nb.sav and nb_pickle.pickle locally\n",
    "\n",
    "#Original credit to \"Junaed Younus Khan\" : \n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "with open('datasets/train.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    next(csv_reader)\n",
    "\n",
    "    # words = []\n",
    "    # c = len(csv_reader)\n",
    "    for line in csv_reader:\n",
    "        texts.append(line[0])\n",
    "        if line[1] == 'FALSE':\n",
    "            labels.append(1)\n",
    "        elif line[1] == 'TRUE':\n",
    "            labels.append(0)\n",
    "\n",
    "with open('datasets/test.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    next(csv_reader)\n",
    "\n",
    "    # words = []\n",
    "    # c = len(csv_reader)\n",
    "    for line in csv_reader:\n",
    "        texts.append(line[0])\n",
    "        if line[1] == 'FALSE':\n",
    "            labels.append(1)\n",
    "        elif line[1] == 'TRUE':\n",
    "            labels.append(0)\n",
    "\n",
    "# print(texts)\n",
    "'''\n",
    "texts = [\n",
    "        \"good movies\", \"not a good movie\", \"did not like\",\n",
    "        \"i like it\", \"good one\"\n",
    "\n",
    "]\n",
    "\n",
    "print(texts)\n",
    "labels = [\n",
    "        \"1\",\"0\",\"0\",\"1\",\"1\"\n",
    "\n",
    "\n",
    "]\n",
    "'''\n",
    "tfidf = TfidfVectorizer(min_df=2, max_df=0.5,\n",
    "                        ngram_range=(1, 1), stop_words='english')\n",
    "features = tfidf.fit_transform(texts)\n",
    "pd.DataFrame(\n",
    "    features.todense(),\n",
    "    columns=tfidf.get_feature_names_out()\n",
    ")\n",
    "\n",
    "\n",
    "features = features.toarray()\n",
    "\n",
    "\n",
    "# print(tfidf.get_feature_names())\n",
    "\n",
    "# x_train, x_test, y_train, y_test = tts(features, labels, test_size=0.2)\n",
    "\n",
    "x_train = features[0:10240]\n",
    "y_train = labels[0:10240]\n",
    "\n",
    "x_test = features[10240:]\n",
    "y_test = labels[10240:]\n",
    "\n",
    "\n",
    "# classifiers\n",
    "clf_nb = MultinomialNB()\n",
    "\n",
    "# clf_svm = svm.SVC(kernel='linear')\n",
    "# clf_lr = LogisticRegression()\n",
    "\n",
    "#########\n",
    "\n",
    "\n",
    "# model save\n",
    "print(\"training start.........\")\n",
    "print(\".\")\n",
    "print(\"nb start\")\n",
    "clf_nb.fit(x_train, y_train)\n",
    "filename = 'nb.sav'\n",
    "pickle.dump(clf_nb, open(filename, 'wb'))\n",
    "print(\"nb done\")\n",
    "\n",
    "\n",
    "######################\n",
    "\n",
    "\n",
    "##########################\n",
    "filename = 'nb.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "# result = loaded_model.score(X, y)\n",
    "# print(result)\n",
    "\n",
    "\n",
    "pred = loaded_model.predict(x_test)\n",
    "\n",
    "print(\"###################\")\n",
    "print(\".\")\n",
    "print(\"test results: \")\n",
    "print(\"---------nb---------------------\")\n",
    "print(\"test_accuracy: \")\n",
    "print(f\"{accuracy_score(y_test, pred):.4f}\")\n",
    "\n",
    "print(\"test_precision: \")\n",
    "print(f'{precision_score(y_test, pred, average=\"weighted\"):.4f}')\n",
    "\n",
    "print(\"test_recall: \")\n",
    "print(f'{recall_score(y_test, pred, average=\"weighted\"):.4f}')\n",
    "\n",
    "print(\"test_f1 \")\n",
    "print(f'{f1_score(y_test, pred, average=\"weighted\"):.4f}')\n",
    "\n",
    "\n",
    "filename = 'nb_pickle.pickle'\n",
    "pickle.dump((y_test, pred), open(filename, 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion of the above code: \n",
    "\n",
    "These are the results from one run:\n",
    "\n",
    "test_accuracy: \n",
    "0.5966\n",
    "\n",
    "\n",
    "test_precision: \n",
    "0.5973\n",
    "\n",
    "\n",
    "test_recall: \n",
    "0.5966\n",
    "\n",
    "\n",
    "test_f1 \n",
    "0.5758\n",
    "\n",
    "\n",
    "As you can see, these results aren't very good. On the smaller dataset \"liar\" (in their code it is just called test.csv and train.csv) Naive Bayes with very simple Tf-idf doesn't perform well at all. We feel that this code sample is representative of the other Naive Bayes implementations in the paper because they used the same code almost exactly for the three different datasets, Liar, Fake/Real, and the Combined Corpus. They used Tf-idf without any feature engineering or preprocessing, which leaves room for improvement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gkHhku9h-j2"
   },
   "source": [
    "*********************************************************************************************************************\n",
    "### Contribution  Code :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is random_forest_liar.py, which can be found under code/random_forest_liar.py in this repo. This isn't the only python file of our implementation, but it is the one which can be directly compared to the code sample from the original paper which was included above. This code takes the same dataset, the Liar dataset and trains random forest on it. In addition, there are feature engineering techniques beyond vectorization to transform the data which were not present in the original paper's code.\n",
    "\n",
    "Specifically, SMOTE (Synthetic Minority Over-Sampling) is used to address the imbalance between false and true samples, as the Liar dataset has nearly 40% more \"False\" labelled values than \"True\". \n",
    "\n",
    "Other features were extracted from the raw text data, as was done for other ML models (but not Naive Bayes) in the paper, such as avg_word_length, unique_word_count, etc. \n",
    "\n",
    "In addition to vectorization (count vectorization, not tf-idf), these extracted features helped to improve the performance with some extra insight of a relatively simple ML model random forest, which offers an alternative to the ones used in the paper originally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Sample:\n",
      "                                           Statement  Label\n",
      "0  Says the Annies List political group supports ...      0\n",
      "1  When did the decline of coal start? It started...      1\n",
      "2  Hillary Clinton agrees with John McCain \"by vo...      1\n",
      "3  Health care reform legislation is likely to ma...      0\n",
      "4  The economic turnaround started at the end of ...      1\n",
      "\n",
      "Original class distribution: [4488 5752]\n",
      "Class distribution after SMOTE: [5752 5752]\n",
      "\n",
      "Model Performance:\n",
      "Accuracy: 0.6103\n",
      "Precision: 0.6151\n",
      "Recall: 0.7504\n",
      "F1 Score: 0.6760\n",
      "ROC AUC: 0.6446\n",
      "\n",
      "\n",
      "Model saved as 'fake_news_detector_rf_lier.pickle'\n"
     ]
    }
   ],
   "source": [
    "#Note : this saves \"fake_news_detector_rf_liar.pickle\" locally\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data():\n",
    "    \n",
    "    train_df = pd.read_csv('datasets/train.csv')\n",
    "    test_df = pd.read_csv('datasets/test.csv')\n",
    "    \n",
    "    #encode the target \n",
    "    label_map = {False: 0, True: 1}\n",
    "    train_df['Label'] = train_df['Label'].map(label_map)\n",
    "    test_df['Label'] = test_df['Label'].map(label_map)\n",
    "    print(f\"Data Sample:\\n{train_df.head()}\\n\")\n",
    "    return train_df, test_df\n",
    "\n",
    "# Feature engineering \n",
    "def engineer_features(df):\n",
    "    \n",
    "    \n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    # Counts of characters, words, unique words, etc\n",
    "    features['char_count'] = df['Statement'].apply(len).astype(np.float64)\n",
    "    features['word_count'] = df['Statement'].apply(lambda x: len(str(x).split())).astype(np.float64)\n",
    "    features['unique_word_count'] = df['Statement'].apply(lambda x: len(set(str(x).lower().split()))).astype(np.float64)\n",
    "    # this is a calculation of how many words are unique in the sample \n",
    "    features['unique_word_ratio'] = (features['unique_word_count'] / (features['word_count'] + 1)).astype(np.float64)\n",
    "    \n",
    "    #average word length\n",
    "    features['avg_word_length'] = df['Statement'].apply(\n",
    "        lambda x: np.mean([len(word) for word in str(x).split()]) if len(str(x).split()) > 0 else 0\n",
    "    ).astype(np.float64)\n",
    "    \n",
    "    #  Sentence features using regular expressions\n",
    "    import re\n",
    "    features['sentence_count'] = df['Statement'].apply(lambda x: len(re.split(r'[.!?]+', str(x)))).astype(np.float64)\n",
    "    features['avg_sentence_length'] = (features['word_count'] / (features['sentence_count'] + 1)).astype(np.float64)\n",
    "    \n",
    "    # Special characters like exclamation (might indicate a sensational article title for example)\n",
    "    features['exclamation_count'] = df['Statement'].apply(lambda x: str(x).count('!')).astype(np.float64)\n",
    "    features['question_count'] = df['Statement'].apply(lambda x: str(x).count('?')).astype(np.float64)\n",
    "    features['capital_count'] = df['Statement'].apply(lambda x: sum(1 for c in str(x) if c.isupper())).astype(np.float64)\n",
    "    features['capital_ratio'] = (features['capital_count'] / (features['char_count'] + 1)).astype(np.float64)\n",
    "    \n",
    "    # flags for different things like the inclusion of quotes, or if there is a source cited \n",
    "    features['has_number'] = df['Statement'].apply(lambda x: float(bool(re.search(r'\\d', str(x)))))\n",
    "    features['has_quote'] = df['Statement'].apply(lambda x: float(bool(re.search(r'\\\"|\\\"|\\'|\\'', str(x)))))\n",
    "    features['has_source'] = df['Statement'].apply(lambda x: float(bool(re.search(r'\\bsource\\b|\\baccording\\b', str(x).lower()))))\n",
    "\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Text vectorization function with sklearn CountVectorizer\n",
    "def vectorize_text(train_df, test_df, max_features=2000):\n",
    "   \n",
    "    #use bigram and unigram\n",
    "    vectorizer = CountVectorizer(\n",
    "        max_features=max_features, \n",
    "        ngram_range=(1, 2),\n",
    "        stop_words='english',\n",
    "        min_df=5\n",
    "    )\n",
    "    \n",
    "    # Fit and transform\n",
    "    X_text_train = vectorizer.fit_transform(train_df['Statement'])\n",
    "    X_text_test = vectorizer.transform(test_df['Statement'])\n",
    "    \n",
    "    return X_text_train, X_text_test, vectorizer\n",
    "\n",
    "# Combine the vectorized text with the added derived features\n",
    "def combine_features(X_text_train, X_text_test, X_stats_train, X_stats_test):\n",
    "   \n",
    "    # csr to compress the spare matrix from vectorization\n",
    "    X_stats_train_csr = csr_matrix(X_stats_train.astype(np.float64).values)\n",
    "    X_stats_test_csr = csr_matrix(X_stats_test.astype(np.float64).values)\n",
    "    \n",
    "    # combine all the engineered features and the vectorized \n",
    "    X_train_combined = hstack([X_text_train, X_stats_train_csr])\n",
    "    X_test_combined = hstack([X_text_test, X_stats_test_csr])\n",
    "\n",
    "    \n",
    "    return X_train_combined, X_test_combined\n",
    "\n",
    "# smote for handling class imbalance\n",
    "def apply_smote(X_train, y_train):\n",
    "\n",
    "    smote = SMOTE(random_state=22)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(f\"Original class distribution: {np.bincount(y_train)}\")\n",
    "    print(f\"Class distribution after SMOTE: {np.bincount(y_train_resampled)}\")\n",
    "    return X_train_resampled, y_train_resampled\n",
    "\n",
    "# Random Forest classifier\n",
    "def train_random_forest(X_train, y_train):\n",
    "    \n",
    "    #params were chosen with GridSearchCV, though not incldued because of the long run-time\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=1,\n",
    "        max_depth=None,\n",
    "        class_weight='balanced',\n",
    "        random_state=22,\n",
    "        n_jobs=-1 \n",
    "    )\n",
    "    \n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    return rf_model\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] #get probabilities for both true and false, needed for roc\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nModel Performance:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "def main():  \n",
    "    \n",
    "    # Load and preprocess data\n",
    "    train_df, test_df = load_data()\n",
    "    \n",
    "    # Engineer features\n",
    "    train_features = engineer_features(train_df)\n",
    "    test_features = engineer_features(test_df)\n",
    "    \n",
    "    # Vectorize text\n",
    "    X_text_train, X_text_test, vectorizer = vectorize_text(train_df, test_df, max_features=2000)\n",
    "    \n",
    "    # Combine features\n",
    "    X_train, X_test = combine_features(X_text_train, X_text_test, train_features, test_features)\n",
    "    y_train = train_df['Label'].values\n",
    "    y_test = test_df['Label'].values\n",
    "    \n",
    "    # Apply SMOTE\n",
    "    X_train_resampled, y_train_resampled = apply_smote(X_train, y_train)\n",
    "    \n",
    "    # Train Random Forest\n",
    "    rf_model = train_random_forest(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    results = evaluate_model(rf_model, X_test, y_test)\n",
    "    \n",
    "    \n",
    "    # Save the model\n",
    "    import pickle\n",
    "    with open('fake_news_detector_rf_liar.pickle', 'wb') as model_file:\n",
    "        pickle.dump({'model': rf_model, 'vectorizer': vectorizer}, model_file)\n",
    "    \n",
    "    print(\"\\nModel saved as 'fake_news_detector_rf_liar.pickle'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YdFCgWoh-j3"
   },
   "source": [
    "### Results :\n",
    "*******************************************************************************************************************************\n",
    "\n",
    "Seen above, the results are ok. They are marginally better than with Naive Bayes on the \"Liar\" dataset.\n",
    "\n",
    "Accuracy: 0.5966 -> 0.6103\n",
    "\n",
    "Precision: 0.5973 -> 0.6151\n",
    "\n",
    "Recall: 0.5966 -> 0.7504\n",
    "\n",
    "F1 Score: 0.5758 -> 0.6760\n",
    "\n",
    "ROC AUC: (not measured) -> 0.6446 \n",
    "\n",
    "However, this is not the only dataset from the paper that we tested Random Forest on. \n",
    "\n",
    "In the file from this repo: code/random_forest_fake_or_real_and_combined_corupus.py, we train Random Forest using the same feature engineering as shown above but on much larger datasets: fake or real dataset, and the combined corpus dataset. These datasets differ from the liar dataset previously used because they aren't raw data- they are preprocessed from the study. They contain meta data as columns like average word length, etc. This combined corpus dataset is what the study claims Naive Bayes got 93% accuracy training on. This dataset is around 86,000 rows, each representing a news headline or article categorized as true or false.\n",
    "\n",
    "These are the results from running our same random Forest methodology on the other two, larger datasets:\n",
    "\n",
    "##### Best Parameters for fake or real Dataset:\n",
    "\n",
    "class_weight: None\n",
    "\n",
    "max_depth: 10\n",
    "\n",
    "min_samples_leaf: 1\n",
    "\n",
    "min_samples_split: 10\n",
    "\n",
    "n_estimators: 100\n",
    "\n",
    "Best F1 Score: 0.7187\n",
    "\n",
    "\n",
    "\n",
    "##### Model Performance on fake or real Dataset:\n",
    "\n",
    "Accuracy: 0.7524\n",
    "\n",
    "Precision: 0.7290\n",
    "\n",
    "Recall: 0.7945\n",
    "\n",
    "F1 Score: 0.7604\n",
    "\n",
    "ROC AUC: 0.8277\n",
    "\n",
    "##### Best params for combined corpus:\n",
    "\n",
    "Best Parameters:\n",
    "\n",
    "class_weight: None\n",
    "\n",
    "max_depth: 20\n",
    "\n",
    "min_samples_leaf: 2\n",
    "\n",
    "min_samples_split: 10\n",
    "\n",
    "n_estimators: 200\n",
    "\n",
    "Best F1 Score: 0.7933\n",
    "\n",
    "##### Combine corpus dataset performance: \n",
    "\n",
    "Model Performance:\n",
    "\n",
    "Accuracy: 0.7765\n",
    "\n",
    "Precision: 0.8421\n",
    "\n",
    "Recall: 0.8264\n",
    "\n",
    "F1 Score: 0.8342\n",
    "\n",
    "ROC AUC: 0.8282\n",
    "\n",
    "\n",
    "#### Observations :\n",
    "*******************************************************************************************************************************\n",
    "*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3JVj9dKh-j3"
   },
   "source": [
    "### Conclusion and Future Direction :\n",
    "*******************************************************************************************************************************\n",
    "#### Learnings :\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "#### Results Discussion :\n",
    "\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "#### Limitations :\n",
    "\n",
    "\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "#### Future Extension :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATXtFdtBh-j4"
   },
   "source": [
    "# References:\n",
    "\n",
    "[1]:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQnMSAf-h-j4"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
